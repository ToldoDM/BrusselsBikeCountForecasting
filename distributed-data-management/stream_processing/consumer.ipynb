{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373bf3e6",
   "metadata": {},
   "source": [
    "# Stream Processing\n",
    "## Goal\n",
    "The producer should first download all historical measurements between 2018-12-06 and 2023-03-31, and then send batches of measurements in regular time intervals. Specifically, every $∆$ seconds, the producer sends a batch of measurements corresponding to all timestamps within a time period $Π$. For example, if $Π$ = 30 days, and ∆ = 5 seconds, the producer will send one month’s worth of data for each sensor every 5 seconds. The producer should be parameterizable by $∆$ and $Π$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3db6c",
   "metadata": {},
   "source": [
    "## Common imports and Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from pyspark import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import combinations\n",
    "from pyspark.sql.types import StructField, IntegerType, FloatType\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 ' + \\\n",
    "                                    '--conf spark.driver.memory=1g  pyspark-shell '\n",
    "streaming_path = \"streaming\"\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9a490",
   "metadata": {},
   "source": [
    "## Spark session construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/07 23:58:48 WARN Utils: Your hostname, toldo-Victus resolves to a loopback address: 127.0.1.1; using 192.168.122.1 instead (on interface virbr0)\n",
      "23/05/07 23:58:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/07 23:58:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 23:58:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"stream_processing\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Streaming construction\n",
    "We connect a `StreamingContext` object to the `producer` notebook, on `localhost` with port $9999$. The `producer` notebook has to be launched first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method definitions\n",
    "Since the message sent from the producer is in text format, this method converts its values back to the original format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the transform func\n",
    "def convert_to_number(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next method is used to load all streaming data, since from batch to batch we have only the most recent data, and then pivot the dataframe.\n",
    "The function returns a pivoted dataframe with a column for each sensor, and a list of all the combination of sensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_and_process_main_df(spk):\n",
    "    # Read the CSV file into a Spark DataFrame\n",
    "    batch_data_at_t = spk.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(streaming_path)\n",
    "\n",
    "    # pivot the dataframe\n",
    "    pivot_df = batch_data_at_t.groupBy(\"Date\", \"Time gap\").pivot(\"Sensor\").agg(first(\"Count\").alias(\"Count\"),\n",
    "                                                                               first(\"Average speed\").alias(\n",
    "                                                                                   \"Average speed\"))\n",
    "\n",
    "    #Sort by date and time gap\n",
    "    pivot_df = pivot_df.withColumn(\"Time gap\", col(\"Time gap\").cast(\"int\")).sort(\"Date\", \"Time gap\")\n",
    "\n",
    "    # Store all columns names\n",
    "    columns = pivot_df.columns\n",
    "\n",
    "    # Store only \"count\" columns and timestamp\n",
    "    count_columns = [c for c in columns if c.endswith(\"_Count\")]\n",
    "    # Create all possible combinations of sensors\n",
    "    pairs = list(combinations(count_columns, 2))\n",
    "\n",
    "    # Keep timestamp\n",
    "    count_columns.append(\"Date\")\n",
    "    count_columns.append(\"Time gap\")\n",
    "\n",
    "    # Filter out the dataframe to only keep those\n",
    "    return pivot_df.select(count_columns), pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method is used to calculate the Pearson coefficient between two sensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the function to calculate Pearson correlation coefficient with time component\n",
    "def pearson_corr_coeff_time(count_df, i, j):\n",
    "    # Assuming i and j are the names of two columns\n",
    "    col_i = col(i)\n",
    "    col_j = col(j)\n",
    "\n",
    "    # We define the window function to take in consideration all the data from the current line to the beginning\n",
    "    w = Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    # Compute the window function count for column i and j\n",
    "    ci_count = count(i).over(w)\n",
    "    cj_count = count(j).over(w)\n",
    "\n",
    "    # Use a sub-query to calculate the various column needed for the calculation of the Pearson coefficient\n",
    "    # We modify the name of the sensor column to i and j\n",
    "    subquery = count_df.select('Date', 'Time gap', col_i.alias('i'), col_j.alias('j'),\n",
    "                               ci_count.alias('ci_count'),\n",
    "                               cj_count.alias('cj_count'))\n",
    "    # We define columns that are the sum of the passage of bikes of the sensor i/j\n",
    "    subquery = subquery.select('Date', 'Time gap', 'i', 'j', sum('i').over(w).alias('ci_total'),\n",
    "                               sum('j').over(w).alias('cj_total'), 'ci_count', 'cj_count')\n",
    "    # We define columns that are the mean of the sensor i/j\n",
    "    subquery = subquery.select('Date', 'Time gap', 'i', 'j', 'ci_total', 'cj_total', 'ci_count', 'cj_count',\n",
    "                               (col('ci_total') / col('ci_count')).alias('ci_mean'),\n",
    "                               (col('cj_total') / col('cj_count')).alias('cj_mean'))\n",
    "    # We define columns that are the numerator and the two part of the denominator of the Pearson coefficient formula\n",
    "    subquery = subquery.select('Date', 'Time gap', 'i', 'j', 'ci_total', 'cj_total', 'ci_count', 'cj_count', 'ci_mean',\n",
    "                               'cj_mean',\n",
    "                               (sum((col('i') - col('ci_mean')) * (col('j') - col('cj_mean'))).over(w)).alias(\n",
    "                                   'numerator'),\n",
    "                               ((sum((col('i') - col('ci_mean')) ** 2).over(w)) ** 0.5).alias('den1'),\n",
    "                               ((sum((col('j') - col('cj_mean')) ** 2).over(w)) ** 0.5).alias('den2'))\n",
    "\n",
    "    # Compute the Pearson correlation coefficient for each date and time gap\n",
    "    rij = subquery.select(\"Date\", \"Time gap\",\n",
    "                          when((col(\"den1\") == 0) | (col(\"den2\") == 0), 0).otherwise(\n",
    "                              col(\"numerator\") / (col(\"den1\") * col(\"den2\"))))\n",
    "\n",
    "    # Renaming columns for clarity\n",
    "    i = i.rstrip(\"_Count\")\n",
    "    j = j.rstrip(\"_Count\")\n",
    "    rij = rij.withColumnRenamed(rij.columns[2], i + \"_\" + j)\n",
    "\n",
    "    return rij"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method handles the reception of the batch by converting the message to a list of data, creating a dataframe, and saving it in CSV format.\n",
    "The saved data is then loaded to calculate the corresponding Pearson coefficient and for every batch received it will print the top 5 most correlated sensors at time $t$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to process each RDD\n",
    "def process_rdd(_, rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        # Get spark session\n",
    "        spk = SparkSession.builder.master(\"local[*]\").appName(\"stream_processing\").getOrCreate()\n",
    "\n",
    "        # Define the schema for the DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"Date\", StringType(), True),\n",
    "            StructField(\"Time gap\", IntegerType(), True),\n",
    "            StructField(\"Sensor\", StringType(), True),\n",
    "            StructField(\"Count\", FloatType(), True),\n",
    "            StructField(\"Average speed\", FloatType(), True)\n",
    "        ])\n",
    "\n",
    "        data_list = []\n",
    "        # Loop through each row of the message\n",
    "        for i, item in tqdm(enumerate(rdd.collect())):\n",
    "            # Each row is an array, and we need to parse it to get it\n",
    "            # for this we are getting rid of the characters that interfere with the parsing\n",
    "            # ex. [['0','2018-12-06','1','CAT17','0.0','-1.0'], -> ['0','2018-12-06','1','CAT17','0.0','-1.0']\n",
    "            if i == 0:\n",
    "                single_row = ast.literal_eval(item[1:-1])\n",
    "            else:\n",
    "                single_row = ast.literal_eval(item[:-1])\n",
    "            # We iterate through the single row from the second element because the first one is not needed.\n",
    "            # We also convert the elements of the array to int or float if needed\n",
    "            data_list.append(Row(*[convert_to_number(element) for element in single_row[1:]]))\n",
    "\n",
    "        rows_df = spk.createDataFrame(data_list, schema)\n",
    "        # Write csv so it is accessible to everyone\n",
    "        rows_df.write.option(\"header\", \"true\").mode(\"append\").csv(streaming_path)\n",
    "        # Load all the data\n",
    "        main_df, pairs = load_and_process_main_df(spk)\n",
    "        rij_dict = {}\n",
    "        # Calculate the Pearson coefficient for each sensor pair\n",
    "        for pair in tqdm(pairs):\n",
    "            rij = pearson_corr_coeff_time(main_df, pair[0], pair[1])\n",
    "            # We are going to take only the last value to get the most correlated sensor pair\n",
    "            last_row = spk.createDataFrame(rij.tail(1))\n",
    "            for i in last_row.collect():\n",
    "                rij_dict[last_row.columns[2]] = i[last_row.columns[2]]\n",
    "        # Create a spark Dataframe with all the values collected\n",
    "        last_rj_df = spk.createDataFrame(Row(rij_dict))\n",
    "        pdf = last_rj_df.toPandas()\n",
    "        print(\"Top5 most correlated sensors:\")\n",
    "        # Transpose the Dataframe taking only the top 5 row, so is more readable\n",
    "        print(pdf.T.nlargest(5, 0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Socket connection\n",
    "This code block sets up a Spark Streaming application that reads data from a network socket and processes it in batches. The batch_interval variable specifies the time interval (in seconds) for each batch of data. The value must be less or equal to the $delta$ value set in the `producer`.\n",
    "\n",
    "A StreamingContext is then created using the Spark context and the batch interval. Checkpointing is enabled to support stateful transformations.\n",
    "\n",
    "Next, a DStream is created from a network socket using the `socketTextStream()` function. This creates a stream of data that is divided into RDDs, which are then processed using the `process_rdd()` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toldo/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_interval = 5\n",
    "\n",
    "# Get Spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "#Create streaming context, with required batch interval\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "#Checkpointing needed for stateful transforms\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Create a DStream that represents streaming data from a network socket\n",
    "# See https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example\n",
    "dstream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Apply the process_rdd function to each RDD in the DStream\n",
    "dstream.foreachRDD(process_rdd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we start the data consumption. At each `batch_interval`, the data sent by the producer is collected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdcff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51840it [00:00, 60574.50it/s]                                       (0 + 1) / 1]\n",
      "100%|██████████| 153/153 [02:08<00:00,  1.19it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top5 most correlated sensors:\n",
      "                       0\n",
      "CEK049_CJM90    0.807839\n",
      "CB02411_CJM90   0.742815\n",
      "CB1599_CLW239   0.711880\n",
      "CB02411_CEK049  0.701100\n",
      "CB1599_CB2105   0.669843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51840it [00:00, 67802.29it/s]                                       (0 + 1) / 1]\n",
      "100%|██████████| 153/153 [02:01<00:00,  1.25it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top5 most correlated sensors:\n",
      "                       0\n",
      "CEK049_CJM90    0.807839\n",
      "CB02411_CJM90   0.742815\n",
      "CB1599_CLW239   0.711880\n",
      "CB02411_CEK049  0.701100\n",
      "CB1599_CB2105   0.669843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51840it [00:00, 68192.96it/s]                                       (0 + 1) / 1]\n",
      "100%|██████████| 153/153 [02:02<00:00,  1.25it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top5 most correlated sensors:\n",
      "                       0\n",
      "CEK049_CJM90    0.807839\n",
      "CB02411_CJM90   0.742815\n",
      "CB1599_CLW239   0.711880\n",
      "CB02411_CEK049  0.701100\n",
      "CB1599_CB2105   0.669843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51840it [00:00, 66810.01it/s]                                       (0 + 1) / 1]\n",
      "100%|██████████| 153/153 [02:08<00:00,  1.19it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top5 most correlated sensors:\n",
      "                       0\n",
      "CEK049_CJM90    0.807839\n",
      "CB02411_CJM90   0.742815\n",
      "CB1599_CLW239   0.711880\n",
      "CB02411_CEK049  0.701100\n",
      "CB1599_CB2105   0.669843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/toldo/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/toldo/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m ssc\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Await termination\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mssc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/pyspark/streaming/context.py:239\u001B[0m, in \u001B[0;36mStreamingContext.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;124;03mWait for the execution to stop.\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;124;03m    time to wait in seconds\u001B[39;00m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jssc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jssc\u001B[38;5;241m.\u001B[39mawaitTerminationOrTimeout(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n",
      "File \u001B[0;32m~/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m~/Desktop/ULB/BigData/big_data_scalable/venv/lib/python3.10/site-packages/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "\n",
    "# Await termination\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To stop the data consumption, we stop the data stream object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
